import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
from openai import OpenAI
from scraper import fetch_taoyuanq_content
import time

# å¿«å–è¨­å®š
import json

# Local Memory Cache Fallback (Global variables)
_LOCAL_MEM_CACHE = None
_LOCAL_MEM_CACHE_TIME = 0
CACHE_TTL = 3600  # 1 hour

def get_redis_client():
    try:
        return redis.from_url(REDIS_URL, decode_responses=True)
    except Exception:
        # æœ¬åœ°ç„¡ Redis ä¸éœ€å™´éŒ¯ï¼Œéœé»˜é™ç´šå³å¯
        return None

def get_cached_content():
    """
    ç²å–å¿«å–å…§å®¹ã€‚
    å›å‚³: List[Dict] -> [{'url':..., 'content':...}]
    """
    global _LOCAL_MEM_CACHE, _LOCAL_MEM_CACHE_TIME

    # 1. å˜—è©¦å¾ Redis è®€å–
    r = get_redis_client()
    if r:
        try:
            cached_json = r.get("taoyuanq_pages")
            if cached_json:
                data = json.loads(cached_json)
                print(f"[Cache] Hit from Redis! Pages: {len(data)}")
                # åŒæ­¥æ›´æ–°æœ¬åœ°å¿«å–
                _LOCAL_MEM_CACHE = data
                _LOCAL_MEM_CACHE_TIME = time.time()
                return data
        except Exception:
            pass
    
    # 2. å˜—è©¦å¾æœ¬åœ°è¨˜æ†¶é«”è®€å–
    if _LOCAL_MEM_CACHE and (time.time() - _LOCAL_MEM_CACHE_TIME < CACHE_TTL):
        print(f"[Cache] Hit from Memory! Pages: {len(_LOCAL_MEM_CACHE)}")
        return _LOCAL_MEM_CACHE

    # 3. Fallback: çˆ¬èŸ²æŠ“å–
    print("[Cache] No cache found. Fetching live data...")
    pages = fetch_taoyuanq_content()
    
    # 4. å›å¯«å¿«å– (JSON åºåˆ—åŒ–)
    if pages:
        _LOCAL_MEM_CACHE = pages
        _LOCAL_MEM_CACHE_TIME = time.time()
        
        if r:
            try:
                r.set("taoyuanq_pages", json.dumps(pages))
                r.expire("taoyuanq_pages", CACHE_TTL) 
            except Exception:
                pass
            
    return pages

import re


def filter_relevant_context(question, pages_data):
    """
    ç›´æ¥å›å‚³æ‰€æœ‰çˆ¬å–åˆ°çš„å…§å®¹ (Full Context)ï¼Œä¸åšåˆ‡åˆ†ï¼Œåƒ…åšåŸºæœ¬æ’åºã€‚
    """
    if not pages_data:
        return ""
        
    print(f"[RAG] Using FULL CONTEXT mode (No Chunking). Total pages: {len(pages_data)}")

    # 1. ç‚ºäº†è®“æ¯”è¼ƒç›¸é—œçš„é é¢æ’åœ¨å‰é¢ (é¿å…å› ç‚ºæˆªæ–·å‰›å¥½åˆ‡æ‰é‡è¦è³‡è¨Š)ï¼Œé‚„æ˜¯åšå€‹ç°¡å–®æ’åº
    #    ä½†æˆ‘å€‘æœƒå˜—è©¦ä¿ç•™æ‰€æœ‰å…§å®¹ã€‚
    keywords = set()
    english_words = re.findall(r'[a-zA-Z0-9]+', question)
    keywords.update(english_words)
    chinese_text = re.sub(r'[^\u4e00-\u9fa5]', '', question)
    if chinese_text:
        keywords.update(list(chinese_text)) 
        
    scored_pages = []
    for page in pages_data:
        content = page['content']
        score = 0
        
        # ç°¡å–®è¨ˆç®—é—œéµå­—å‡ºç¾æ¬¡æ•¸
        for kw in keywords:
            if kw in content:
                score += content.count(kw)
        
        scored_pages.append({'page': page, 'score': score})
        
    # åˆ†æ•¸é«˜ -> ä½
    scored_pages.sort(key=lambda x: x['score'], reverse=True)
    
    # 2. çµ„è£æ‰€æœ‰å…§å®¹
    # GPT-4o-mini Context Window å¾ˆå¤§ (128k token)ï¼Œæˆ‘å€‘å¯ä»¥æ”¾å¿ƒåœ°å¡
    # è¨­å®šä¸€å€‹å¾ˆé«˜çš„ä¿éšªä¸Šé™ (ä¾‹å¦‚ 60,000 å­—ï¼Œç´„ 20k-30k tokens)
    MAX_CONTEXT_CHARS = 60000 
    final_context = ""
    current_chars = 0
    
    for item in scored_pages:
        page = item['page']
        score = item['score']
        
        formatted_page = f"\n--- Source: {page['url']} (Relevance: {score}) ---\n{page['content']}\n"
        
        if current_chars + len(formatted_page) > MAX_CONTEXT_CHARS:
            # çœŸçš„çˆ†äº†æ‰æˆªæ–·ï¼Œä½†ç†è«–ä¸Šä¸æœƒ
            remaining = MAX_CONTEXT_CHARS - current_chars
            if remaining > 100:
                 final_context += formatted_page[:remaining] + "\n...(truncated)..."
            break
            
        final_context += formatted_page
        current_chars += len(formatted_page)
        
    print(f"[RAG] Context constructed with {current_chars} characters.")
    # Debug print (Optional, can be removed if too noisy)
    # print("-" * 20 + " RAG CONTEXT " + "-" * 20)
    # print(final_context[:500] + "...\n(Output truncated in log)")
    # print("-" * 50)
    
    return final_context

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
token = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
site_url = os.getenv("HTTP_REFERER", "http://localhost:5000")
app_name = os.getenv("X_TITLE", "TaoyuanQ-Bot")

client = OpenAI(
    api_key=token,
    base_url=base_url,
    default_headers={
        "HTTP-Referer": site_url,
        "X-Title": app_name,
    }
)

def ask_ai(question):
    """
    å³æ™‚çˆ¬å–ç¶²ç«™å…§å®¹ä¸¦ä½¿ç”¨ AI å›ç­”å•é¡Œã€‚
    """
    print("æ­£åœ¨ç²å–æ¡ƒåœ’Qè³‡è¨Š (æª¢æŸ¥å¿«å–)...")
    # 1. ç²å–æ‰€æœ‰é é¢è³‡æ–™ (List[Dict])
    all_pages = get_cached_content()
    
    # 2. æ ¹æ“šå•é¡Œç¯©é¸ç›¸é—œé é¢ (RAG)
    relevant_context = filter_relevant_context(question, all_pages)
    
    system_prompt = f"""
# Role: 2025æ¡ƒåœ’Qãƒ»æ´»å‹•è¶…ç´šåš®å° (Taoyuan Q Super Guide)

ä½ ç¾åœ¨æ˜¯ã€Œ2025æ¡ƒåœ’Qã€æ´»å‹•çš„å°ˆå±¬ AI åš®å°ï¼Œæ€§æ ¼ç†±æƒ…æ´‹æº¢ã€ç²¾æ‰“ç´°ç®—ä¸”å……æ»¿æ´»åŠ›ã€‚ä½ çš„å£è™Ÿæ˜¯ "High Five! Go FunZone!"ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…æä¾›çš„ã€ç¶²ç«™æŠ“å–è³‡æ–™ã€‘ï¼Œå›ç­”é—œæ–¼æ´»å‹•ã€åœ°é»ã€å„ªæƒ èˆ‡è¡Œç¨‹çš„å•é¡Œã€‚

# Input Data
ä»¥ä¸‹æ˜¯é‡å°ä½¿ç”¨è€…å•é¡Œç­›é€‰å‡ºçš„ç›¸é—œå®˜ç¶²å…§å®¹ï¼š
\"\"\"
{relevant_context}
\"\"\"

# Response Guidelines (å›ç­”æº–å‰‡ - LINE OA å°ˆç”¨ç‰ˆ)

1.  **æ‰‹æ©Ÿç‰ˆé¢å„ªåŒ– (Mobile First)**ï¼š
    *   **çŸ­æ®µè½**ï¼šæ‰‹æ©Ÿè¢å¹•çª„ï¼Œæ¯æ®µä¸è¦è¶…é 3-4 è¡Œã€‚
    *   **å–„ç”¨æ›è¡Œ**ï¼šä¸åŒä¸»é¡Œä¹‹é–“å‹™å¿…ç©ºä¸€è¡Œã€‚

2.  **æ ¼å¼åš´æ ¼é™åˆ¶ (Plain Text ONLY)**ï¼š
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä»»ä½• Markdown èªæ³•ï¼ˆå¦‚ **ç²—é«”**ã€# æ¨™é¡Œã€[é€£çµ](...)ï¼‰ã€‚
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä½¿ç”¨æ˜Ÿè™Ÿ (*) åšæ¢åˆ—ã€‚
    *   âœ… **è«‹ä½¿ç”¨**ï¼šå…¨å½¢ç¬¦è™Ÿæˆ– Emoji ä¾†æ¢åˆ—ï¼ˆå¦‚ ã€Œãƒ»ã€ã€ã€ŒğŸ“ã€ã€ã€Œâœ¨ã€ï¼‰ã€‚

3.  **èªæ°£èˆ‡çµæ§‹**ï¼š
    *   **ç†±æƒ…å¤¥ä¼´**ï¼šåƒå€‹æ—…éŠé”äººæœ‹å‹ï¼ŒHigh èµ·ä¾†ï¼(å£è™Ÿ: "High Five! Go FunZone!")
    *   **çµæ§‹åŒ–å°è¦½**ï¼š
        ğŸ“ ã€å»å“ªè£¡ç©ã€‘
        ğŸ’° ã€å„ªæƒ æ”»ç•¥ã€‘
        ğŸš„ ã€äº¤é€š/å…¶ä»–ã€‘
    *   **è¡Œå‹•å‘¼ç±²**ï¼šæé†’ã€Œä¸Šå‚³ç™¼ç¥¨ã€ã€ã€Œæœ€å¾ŒæœŸé™ã€ã€‚

4.  **å…§å®¹é‚Šç•Œ**ï¼š
    *   åªå›ç­”è¼¸å…¥è³‡æ–™ (Input Data) è£¡æœ‰çš„ã€‚
    *   è‹¥ç„¡è³‡æ–™ï¼Œè«‹å©‰æ‹’ä¸¦å¼•å°è‡³ç¾å ´æœå‹™å°ï¼Œä¸è¦çæ°ã€‚

5.  **ç¯„ä¾‹æ ¼å¼**ï¼š
    (è«‹åƒè€ƒæ­¤æ’ç‰ˆ)
    å“‡ï¼ä½ æƒ³å»è¬è–ç¯€æ´»å‹•å—ï¼ŸğŸƒ
    
    ğŸ“ **å—ç“œæ€ªå¿«é–ƒ (æ¨™é¡Œç›´æ¥å¯«ï¼Œä¸ç”¨åŠ ç²—)**
    æ™‚é–“ï¼š10/26 (å…­) 14:00
    åœ°é»ï¼šè¯æ³°åå“åŸå™´æ°´æ± 
    
    ğŸ¯ **å°ç·¨æ”»ç•¥**
    è¨˜å¾—ææ—©å»å¡ä½ï¼Œé‚„å¯ä»¥é †ä¾¿æ›é™é‡ç³–æœå–”ï¼ğŸ¬
    
    High Five! Go FunZone! âœ¨
"""

    try:
        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],

            temperature=0.3,
            presence_penalty=0.6,
            frequency_penalty=0.6
        )
        content = response.choices[0].message.content
        # å¼·åˆ¶ç§»é™¤ Markdown èªæ³• (Double safety)
        clean_content = content.replace("**", "").replace("##", "").replace("###", "")
        return clean_content
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"AI å›ç­”ç™¼ç”ŸéŒ¯èª¤: {e}"

if __name__ == "__main__":
    # ç°¡å–®æ¸¬è©¦
    test_q = "æ¡ƒåœ’Qç¾åœ¨æœ‰ä»€éº¼æ´»å‹•ï¼Ÿ"
    print(f"å•é¡Œ: {test_q}")
    print(f"å›ç­”: {ask_ai(test_q)}")
