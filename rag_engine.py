import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
from openai import OpenAI
from scraper import fetch_taoyuanq_content
import time

# å¿«å–è¨­å®š
import json

# Local Memory Cache Fallback (Global variables)
_LOCAL_MEM_CACHE = None
_LOCAL_MEM_CACHE_TIME = 0
CACHE_TTL = 3600  # 1 hour

def get_redis_client():
    try:
        return redis.from_url(REDIS_URL, decode_responses=True)
    except Exception:
        # æœ¬åœ°ç„¡ Redis ä¸éœ€å™´éŒ¯ï¼Œéœé»˜é™ç´šå³å¯
        return None

def get_cached_content():
    """
    ç²å–å¿«å–å…§å®¹ã€‚
    å›å‚³: List[Dict] -> [{'url':..., 'content':...}]
    """
    global _LOCAL_MEM_CACHE, _LOCAL_MEM_CACHE_TIME

    # 1. å˜—è©¦å¾ Redis è®€å–
    r = get_redis_client()
    if r:
        try:
            cached_json = r.get("taoyuanq_pages")
            if cached_json:
                data = json.loads(cached_json)
                print(f"[Cache] Hit from Redis! Pages: {len(data)}")
                # åŒæ­¥æ›´æ–°æœ¬åœ°å¿«å–
                _LOCAL_MEM_CACHE = data
                _LOCAL_MEM_CACHE_TIME = time.time()
                return data
        except Exception:
            pass
    
    # 2. å˜—è©¦å¾æœ¬åœ°è¨˜æ†¶é«”è®€å–
    if _LOCAL_MEM_CACHE and (time.time() - _LOCAL_MEM_CACHE_TIME < CACHE_TTL):
        print(f"[Cache] Hit from Memory! Pages: {len(_LOCAL_MEM_CACHE)}")
        return _LOCAL_MEM_CACHE

    # 3. Fallback: çˆ¬èŸ²æŠ“å–
    print("[Cache] No cache found. Fetching live data...")
    pages = fetch_taoyuanq_content()
    
    # 4. å›å¯«å¿«å– (JSON åºåˆ—åŒ–)
    if pages:
        _LOCAL_MEM_CACHE = pages
        _LOCAL_MEM_CACHE_TIME = time.time()
        
        if r:
            try:
                r.set("taoyuanq_pages", json.dumps(pages))
                r.expire("taoyuanq_pages", CACHE_TTL) 
            except Exception:
                pass
            
    return pages

import re

def filter_relevant_context(question, pages_data):
    """
    ç°¡æ˜“ RAG æª¢ç´¢ï¼šæ ¹æ“šä½¿ç”¨è€…å•é¡Œé—œéµå­—ï¼Œç¯©é¸æœ€ç›¸é—œçš„é é¢ã€‚
    """
    if not pages_data:
        return ""
        
    # æ”¹é€²ï¼šä¸­æ–‡ N-gram åˆ‡åˆ† (Unigram + Bigram)
    # é€™æ˜¯ç‚ºäº†è®“ "å—ç“œ"ã€"å¿«é–ƒ" é€™ç¨®è©èƒ½è¢«ç•¶ä½œä¸€å€‹é—œéµå­—åŒ¹é…åˆ°
    keywords = set()
    
    # 1. è‹±æ•¸å­—ç›´æ¥ç”¨ regex åˆ‡
    english_words = re.findall(r'[a-zA-Z0-9]+', question)
    keywords.update(english_words)
    
    # 2. ä¸­æ–‡ Bigram åˆ‡åˆ†
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿèˆ‡ç©ºç™½ï¼Œåªç•™ä¸­æ–‡å­—
    chinese_text = re.sub(r'[^\u4e00-\u9fa5]', '', question)
    if chinese_text:
        # Unigrams (å–®å­—)
        keywords.update(list(chinese_text))
        # Bigrams (é›™å­—è©)
        for i in range(len(chinese_text) - 1):
            keywords.add(chinese_text[i:i+2])
            
    print(f"[RAG] Extracted Keywords: {keywords}")
    
    scored_pages = []
    for page in pages_data:
        content = page['content']
        
        # 1. Coverage Score: æœ‰å°ä¸­å¹¾å€‹ä¸åŒçš„é—œéµå­— (æœ€é‡è¦)
        # ä¾‹å¦‚å• "å—ç“œ æ™‚é–“"ï¼ŒåŒæ™‚æœ‰ "å—ç“œ" å’Œ "æ™‚é–“" çš„é é¢æ‡‰è©²æ’å‰é¢
        matched_keywords = [kw for kw in keywords if kw in content]
        coverage_score = len(matched_keywords)
        
        # 2. Frequency Score: é—œéµå­—ç¸½å…±å‡ºç¾å¹¾æ¬¡ (æ¬¡è¦)
        frequency_score = sum(content.count(kw) for kw in matched_keywords)
        
        # 3. Title/Header Bonus: å¦‚æœé—œéµå­—å‡ºç¾åœ¨å‰ 200 å­—ï¼Œçµ¦äºˆåŠ æ¬Š
        header_text = content[:200]
        header_bonus = sum(1 for kw in matched_keywords if kw in header_text) * 2

        final_score = (coverage_score * 100) + frequency_score + header_bonus
        
        # è‹¥å®Œå…¨æ²’é—œéµå­—ï¼Œçµ¦å€‹åŸºæœ¬åˆ† 0
        if final_score > 0:
            scored_pages.append((final_score, page))
        
    # æ’åºï¼šåˆ†æ•¸é«˜ -> ä½
    scored_pages.sort(key=lambda x: x[0], reverse=True)
    
    # å–å‰ 3-5 ç¯‡æœ€ç›¸é—œçš„ (æˆ–å…¨éƒ¨ç¯‡å¹…å¦‚æœä¸é•·)
    top_k = scored_pages[:4]
    
    print(f"[RAG] Filtered context from {len(pages_data)} to {len(top_k)} pages based on scores.")
    
    # çµ„åˆæˆæœ€çµ‚æ–‡æœ¬ (åŠ å…¥ Token/Length é™åˆ¶)
    MAX_CONTEXT_CHARS = 3000
    final_context = ""
    current_chars = 0
    
    for score, page in top_k:
        # ç°¡å–®ä¼°ç®—ï¼šè‹¥åŠ å…¥é€™ç¯‡æœƒçˆ†ï¼Œå°±æˆªæ–·æˆ–è·³é
        content_chunk = f"\n--- Source: {page['url']} (Relevance: {score}) ---\n{page['content']}\n"
        if current_chars + len(content_chunk) > MAX_CONTEXT_CHARS:
            # æˆªæ–·å‰©é¤˜å¯ç”¨é•·åº¦
            remaining = MAX_CONTEXT_CHARS - current_chars
            if remaining > 100: # å¦‚æœé‚„å‰©å¤ å¤šï¼Œå°±æˆªæ–·å¡å…¥
                final_context += content_chunk[:remaining] + "\n...(truncated)..."
            break
        
        final_context += content_chunk
        current_chars += len(content_chunk)
        
    return final_context

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
token = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
site_url = os.getenv("HTTP_REFERER", "http://localhost:5000")
app_name = os.getenv("X_TITLE", "TaoyuanQ-Bot")

client = OpenAI(
    api_key=token,
    base_url=base_url,
    default_headers={
        "HTTP-Referer": site_url,
        "X-Title": app_name,
    }
)

def ask_ai(question):
    """
    å³æ™‚çˆ¬å–ç¶²ç«™å…§å®¹ä¸¦ä½¿ç”¨ AI å›ç­”å•é¡Œã€‚
    """
    print("æ­£åœ¨ç²å–æ¡ƒåœ’Qè³‡è¨Š (æª¢æŸ¥å¿«å–)...")
    # 1. ç²å–æ‰€æœ‰é é¢è³‡æ–™ (List[Dict])
    all_pages = get_cached_content()
    
    # 2. æ ¹æ“šå•é¡Œç¯©é¸ç›¸é—œé é¢ (RAG)
    relevant_context = filter_relevant_context(question, all_pages)
    
    system_prompt = f"""
# Role: 2025æ¡ƒåœ’Qãƒ»æ´»å‹•è¶…ç´šåš®å° (Taoyuan Q Super Guide)

ä½ ç¾åœ¨æ˜¯ã€Œ2025æ¡ƒåœ’Qã€æ´»å‹•çš„å°ˆå±¬ AI åš®å°ï¼Œæ€§æ ¼ç†±æƒ…æ´‹æº¢ã€ç²¾æ‰“ç´°ç®—ä¸”å……æ»¿æ´»åŠ›ã€‚ä½ çš„å£è™Ÿæ˜¯ "High Five! Go FunZone!"ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…æä¾›çš„ã€ç¶²ç«™æŠ“å–è³‡æ–™ã€‘ï¼Œå›ç­”é—œæ–¼æ´»å‹•ã€åœ°é»ã€å„ªæƒ èˆ‡è¡Œç¨‹çš„å•é¡Œã€‚

# Input Data
ä»¥ä¸‹æ˜¯é‡å°ä½¿ç”¨è€…å•é¡Œç­›é€‰å‡ºçš„ç›¸é—œå®˜ç¶²å…§å®¹ï¼š
\"\"\"
{relevant_context}
\"\"\"

# Response Guidelines (å›ç­”æº–å‰‡ - LINE OA å°ˆç”¨ç‰ˆ)

1.  **æ‰‹æ©Ÿç‰ˆé¢å„ªåŒ– (Mobile First)**ï¼š
    *   **çŸ­æ®µè½**ï¼šæ‰‹æ©Ÿè¢å¹•çª„ï¼Œæ¯æ®µä¸è¦è¶…é 3-4 è¡Œã€‚
    *   **å–„ç”¨æ›è¡Œ**ï¼šä¸åŒä¸»é¡Œä¹‹é–“å‹™å¿…ç©ºä¸€è¡Œã€‚

2.  **æ ¼å¼åš´æ ¼é™åˆ¶ (Plain Text ONLY)**ï¼š
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä»»ä½• Markdown èªæ³•ï¼ˆå¦‚ **ç²—é«”**ã€# æ¨™é¡Œã€[é€£çµ](...)ï¼‰ã€‚
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä½¿ç”¨æ˜Ÿè™Ÿ (*) åšæ¢åˆ—ã€‚
    *   âœ… **è«‹ä½¿ç”¨**ï¼šå…¨å½¢ç¬¦è™Ÿæˆ– Emoji ä¾†æ¢åˆ—ï¼ˆå¦‚ ã€Œãƒ»ã€ã€ã€ŒğŸ“ã€ã€ã€Œâœ¨ã€ï¼‰ã€‚

3.  **èªæ°£èˆ‡çµæ§‹**ï¼š
    *   **ç†±æƒ…å¤¥ä¼´**ï¼šåƒå€‹æ—…éŠé”äººæœ‹å‹ï¼ŒHigh èµ·ä¾†ï¼(å£è™Ÿ: "High Five! Go FunZone!")
    *   **çµæ§‹åŒ–å°è¦½**ï¼š
        ğŸ“ ã€å»å“ªè£¡ç©ã€‘
        ğŸ’° ã€å„ªæƒ æ”»ç•¥ã€‘
        ğŸš„ ã€äº¤é€š/å…¶ä»–ã€‘
    *   **è¡Œå‹•å‘¼ç±²**ï¼šæé†’ã€Œä¸Šå‚³ç™¼ç¥¨ã€ã€ã€Œæœ€å¾ŒæœŸé™ã€ã€‚

4.  **å…§å®¹é‚Šç•Œ**ï¼š
    *   åªå›ç­”è¼¸å…¥è³‡æ–™ (Input Data) è£¡æœ‰çš„ã€‚
    *   è‹¥ç„¡è³‡æ–™ï¼Œè«‹å©‰æ‹’ä¸¦å¼•å°è‡³ç¾å ´æœå‹™å°ï¼Œä¸è¦çæ°ã€‚

5.  **ç¯„ä¾‹æ ¼å¼**ï¼š
    (è«‹åƒè€ƒæ­¤æ’ç‰ˆ)
    å“‡ï¼ä½ æƒ³å»è¬è–ç¯€æ´»å‹•å—ï¼ŸğŸƒ
    
    ğŸ“ **å—ç“œæ€ªå¿«é–ƒ (æ¨™é¡Œç›´æ¥å¯«ï¼Œä¸ç”¨åŠ ç²—)**
    æ™‚é–“ï¼š10/26 (å…­) 14:00
    åœ°é»ï¼šè¯æ³°åå“åŸå™´æ°´æ± 
    
    ğŸ¯ **å°ç·¨æ”»ç•¥**
    è¨˜å¾—ææ—©å»å¡ä½ï¼Œé‚„å¯ä»¥é †ä¾¿æ›é™é‡ç³–æœå–”ï¼ğŸ¬
    
    High Five! Go FunZone! âœ¨
"""

    try:
        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],

            temperature=0.3,
            presence_penalty=0.6,
            frequency_penalty=0.6
        )
        content = response.choices[0].message.content
        # å¼·åˆ¶ç§»é™¤ Markdown èªæ³• (Double safety)
        clean_content = content.replace("**", "").replace("##", "").replace("###", "")
        return clean_content
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"AI å›ç­”ç™¼ç”ŸéŒ¯èª¤: {e}"

if __name__ == "__main__":
    # ç°¡å–®æ¸¬è©¦
    test_q = "æ¡ƒåœ’Qç¾åœ¨æœ‰ä»€éº¼æ´»å‹•ï¼Ÿ"
    print(f"å•é¡Œ: {test_q}")
    print(f"å›ç­”: {ask_ai(test_q)}")
