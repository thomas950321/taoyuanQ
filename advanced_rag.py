import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
import shutil
import logging
from typing import List

# LangChain Imports
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_docling import DoclingLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from docling.document_converter import DocumentConverter

# Local Imports
# Reuse the robust link finding logic from scraper.py
from scraper import get_all_links 
import requests

# Setup Logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
CHROMA_PATH = "chroma_db"
API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")

# Global variables for Singleton pattern
_RETRIEVER = None
_VECTORSTORE = None
_DOCSTORE = None

def init_rag_pipeline(rebuild=False):
    """
    Initialize the RAG pipeline components.
    """
    global _RETRIEVER, _VECTORSTORE, _DOCSTORE

    if _RETRIEVER and not rebuild:
        return _RETRIEVER

    logger.info("Initializing RAG Pipeline...")

    # 1. Embeddings Model
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", # Or similar depending on provider
        openai_api_key=API_KEY,
        openai_api_base=BASE_URL
    )

    # 2. Vector Store (Chroma)
    if rebuild and os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        logger.info("Cleared existing vector store.")

    _VECTORSTORE = Chroma(
        collection_name="taoyuanq_docs",
        embedding_function=embeddings,
        persist_directory=CHROMA_PATH
    )

    # 3. Doc Store (InMemory for now, ideally Redis for production)
    _DOCSTORE = InMemoryStore()

    # 4. Splitters
    # Parent splitter: Keep larger context (e.g., full sections)
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    
    # Child splitter: Small chunks for precise retrieval
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)

    # 5. ParentDocumentRetriever
    _RETRIEVER = ParentDocumentRetriever(
        vectorstore=_VECTORSTORE,
        docstore=_DOCSTORE,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
    )
    
    return _RETRIEVER

def fetch_and_process_website():
    """
    Crawl, Parse, Chunk, and Index content.
    """
    base_url = "https://a18.taoyuanq.com/zh"
    logger.info(f"Starting crawl from {base_url}...")

    # 1. Discover all links
    try:
        resp = requests.get(base_url, headers={"User-Agent": "Bot"}, timeout=10)
        all_urls = get_all_links(base_url, resp.text)
        all_urls.add(base_url)
        logger.info(f"Found {len(all_urls)} pages.")
    except Exception as e:
        logger.error(f"Crawling failed: {e}")
        return

    # 2. Docling Conversion & Semantic Chunking
    converter = DocumentConverter()
    
    # We use Semantic Chunker to pre-process before feeding into ParentRetriever
    # Note: ParentRetriever does its own splitting usually, but we can feed it documents 
    # that are already somewhat distinct (by page).
    
    rag_documents = []

    for url in all_urls:
        try:
            logger.info(f"Processing {url}...")
            # Docling conversion
            # Note: DocumentConverter might be slow or blocking.
            conv_res = converter.convert(url)
            markdown_content = conv_res.document.export_to_markdown()
            
            if not markdown_content.strip():
                continue

            # Create a Document object
            doc = Document(
                page_content=markdown_content,
                metadata={"source": url}
            )
            rag_documents.append(doc)
            
        except Exception as e:
            logger.error(f"Failed to process {url}: {e}")

    # 3. Ingest into Retriever
    if rag_documents:
        retriever = init_rag_pipeline()
        logger.info(f"Ingesting {len(rag_documents)} documents into vector store...")
        retriever.add_documents(rag_documents)
        logger.info("Ingestion complete.")
    else:
        logger.warning("No documents to ingest.")

def query_rag_advanced(question, return_usage=False):
    """
    Retrieve context and generate answer.
    """
    retriever = init_rag_pipeline()
    
    # 1. Retrieve relevant docs (Parent Documents)
    # This returns the LARGE chunks (Parents) matched by SMALL chunks (Children)
    docs = retriever.invoke(question)
    
    # Combine content
    context_text = "\n\n".join([f"--- Source: {d.metadata.get('source', 'Unknown')} ---\n{d.page_content}" for d in docs])
    
    logger.info(f"Retrieved {len(docs)} parent documents. Context size: {len(context_text)} chars.")

    # 2. Generate Answer
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        openai_api_key=API_KEY,
        openai_api_base=BASE_URL,
        tags=["experiment_rag"]
    )
    
    system_prompt = f"""
# Role: 2025æ¡ƒåœ’Qãƒ»æ´»å‹•è¶…ç´šåš®å° (Taoyuan Q Super Guide)

ä½ ç¾åœ¨æ˜¯ã€Œ2025æ¡ƒåœ’Qã€æ´»å‹•çš„å°ˆå±¬ AI åš®å°ï¼Œæ€§æ ¼ç†±æƒ…æ´‹æº¢ã€ç²¾æ‰“ç´°ç®—ä¸”å……æ»¿æ´»åŠ›ã€‚ä½ çš„å£è™Ÿæ˜¯ "High Five! Go FunZone!"ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…æä¾›çš„ã€ç¶²ç«™æŠ“å–è³‡æ–™ã€‘ï¼Œå›ç­”é—œæ–¼æ´»å‹•ã€åœ°é»ã€å„ªæƒ èˆ‡è¡Œç¨‹çš„å•é¡Œã€‚

# Input Data
ä»¥ä¸‹æ˜¯é‡å°ä½¿ç”¨è€…å•é¡Œç­›é€‰å‡ºçš„ç›¸é—œå®˜ç¶²å…§å®¹ (å·²ä¿ç•™å®Œæ•´ç¶²é å…§å®¹)ï¼š
\"\"\"
{context_text}
\"\"\"

# Response Guidelines (å›ç­”æº–å‰‡ - LINE OA å°ˆç”¨ç‰ˆ)

1.  **æ‰‹æ©Ÿç‰ˆé¢å„ªåŒ– (Mobile First)**ï¼š
    *   **çŸ­æ®µè½**ï¼šæ‰‹æ©Ÿè¢å¹•çª„ï¼Œæ¯æ®µä¸è¦è¶…é 3-4 è¡Œã€‚
    *   **å–„ç”¨æ›è¡Œ**ï¼šä¸åŒä¸»é¡Œä¹‹é–“å‹™å¿…ç©ºä¸€è¡Œã€‚

2.  **æ ¼å¼åš´æ ¼é™åˆ¶ (Plain Text ONLY)**ï¼š
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä»»ä½• Markdown èªæ³•ï¼ˆå¦‚ **ç²—é«”**ã€# æ¨™é¡Œã€[é€£çµ](...)ï¼‰ã€‚
    *   âŒ **çµ•å°ç¦æ­¢**ï¼šä½¿ç”¨æ˜Ÿè™Ÿ (*) åšæ¢åˆ—ã€‚
    *   âœ… **è«‹ä½¿ç”¨**ï¼šå…¨å½¢ç¬¦è™Ÿæˆ– Emoji ä¾†æ¢åˆ—ï¼ˆå¦‚ ã€Œãƒ»ã€ã€ã€ŒğŸ“ã€ã€ã€Œâœ¨ã€ï¼‰ã€‚

3.  **èªæ°£èˆ‡çµæ§‹**ï¼š
    *   **ç†±æƒ…å¤¥ä¼´**ï¼šåƒå€‹æ—…éŠé”äººæœ‹å‹ï¼ŒHigh èµ·ä¾†ï¼(å£è™Ÿ: "High Five! Go FunZone!")
    *   **çµæ§‹åŒ–å°è¦½**ï¼š
        ğŸ“ ã€å»å“ªè£¡ç©ã€‘
        ğŸ’° ã€å„ªæƒ æ”»ç•¥ã€‘
        ğŸš„ ã€äº¤é€š/å…¶ä»–ã€‘
    *   **è¡Œå‹•å‘¼ç±²**ï¼šæé†’ã€Œä¸Šå‚³ç™¼ç¥¨ã€ã€ã€Œæœ€å¾ŒæœŸé™ã€ã€‚

4.  **å…§å®¹é‚Šç•Œ**ï¼š
    *   åªå›ç­”è¼¸å…¥è³‡æ–™ (Input Data) è£¡æœ‰çš„ã€‚
    *   è‹¥ç„¡è³‡æ–™ï¼Œè«‹å©‰æ‹’ä¸¦å¼•å°è‡³ç¾å ´æœå‹™å°ï¼Œä¸è¦çæ°ã€‚
"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ]
    
    response = llm.invoke(messages)
    if return_usage:
        return response.content, response.response_metadata.get('token_usage', {})
    return response.content

if __name__ == "__main__":
    # Test execution
    # Initialize and rebuild the database for the first run
    init_rag_pipeline(rebuild=True)
    fetch_and_process_website()
    print("Test Query:")
    print(query_rag_advanced("æˆ‘ä»Šå¤©è·Ÿæœ‹å‹ä¸‰å€‹äººå»åƒé£¯é€›è¡—ï¼Œæ‰“ç®—èŠ± 2500 å…ƒï¼Œé€™æ¨£æˆ‘å€‘å¯ä»¥æŠ½çå—ï¼Ÿ"))
